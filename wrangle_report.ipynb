{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wrangling Breakdown of WeRateDogs Twitter Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The gathering Stage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the gathering stage of wrangling data from the WeRateDogs twitter page, the goal is to import three separate sources of the data into a pandas dataframe with python code. \n",
    "\n",
    "The first piece of data is a twitter archive file onhand titled `twitter-archive-enhanced.csv` and is easily read into the `twdf` dataframe.  \n",
    "\n",
    "The second file with predicted dog breed data titled `image_predictions.tsv` is hosted on a Udacity server.  Though this could be downloaded via following a link in a web browser, with replicability of the processs in mind, the requests library is used to download the file into the repository folder.  This file is also read into a pandas dataframe titled `pred_df`.  \n",
    "\n",
    "Finally, since more data on favorites and retweets were required for analysis, the data needed to be constructed into a text file full of JSON data for each WeRateDogs post programatically via the *tweepy* API.  After this is completed, the required favorite_counts and retweet_counts from the text file are easily appended to a dictionary of lists.  This is then converted to the pandas dataframe `json_df`.  \n",
    "\n",
    "With the three dataframes assigned to variables, it is now possible to take a step back and assess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Assessment Stage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is hard to know exactly where to find quality and tidiness issues of data offhand, it sometimes helps just to **visually** assess the data.  To do this, each of the three dataframes (`twdf`, `pred_df`, and `json_df`) are simply displayed in three separate markdown cells.  From this inspection, the following issues become immediatelly apparent:\n",
    "- the four 'doggo', 'floofer', 'pupper' and 'puppo' collumns represent one categorical variable\n",
    "- dog types in the p1, p2 and p3 collumns of the pred_df table are inconsistently capitalized\n",
    "\n",
    "Next, the process of programatically assessing the data starts.  First, in an attempt to overcome the bias of only viewing the head and tail of each dataframe, a random sample of five observations in each dataframe are displayed.  In this view, it becomes clear that the best way to organize the data for this analysis is in a single table without any retweet or in-reply related collumns.\n",
    "\n",
    "Another powerful tool used is the `pd.info()` method.  Through this, every instanve of an incorrect datatype and missing data is observed for each dataframe.  From there, further insights into data where retweeted_status_id data is available are filtered.  It becomes clear that these are in fact retweets and should be dropped. \n",
    "\n",
    "Finally, through the use of for-loops, the occurance of `tweet_id`'s is compared accross tables for matches and completeness.  This confirmes the feasability of performing inner joins that drop the rows with incomplete data.\n",
    "\n",
    "*All thirteen of the quality and tidiness issues are found at the bottom of 'Part II: Assess Data' in `wrangle_act.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cleaning Stage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the assessment of data completed,  it is now possible to begin ticking items of the quality and tidiness lists.  But first, in order to preserve data and have clear variables, the integral first step of copying all three dataframes into the new variables, `master_df`, `json_clean` and `pred_clean` is completed.\n",
    "\n",
    "First, the simple `.astype` method is used to convert the incorrect datatypes of collumns that will be preserved in the final dataframe.\n",
    "\n",
    "The second step quickly gets more complicated.  In order to move the four 'doggo', 'floofer', 'pupper' and 'puppo' collumns into a single one requires replacing 'None' strings with empty, then concatenating all four collumns into one and dropping the old one.  But it does not end there. Some of the resulting `dog_titles` are a combination of two which needs to be iteratively addressed.  These separated into yet another collumn named `second_dog_title` through a couple of tricky yet simple programatic steps. \n",
    "\n",
    "A few more quality issues are easily addressed via a few lines of code prior to the final step that solves six quality and tidiness issues. This step includes dropping the unneccessary collumns in the master_df table, then performing inner joins of all three tables on `tweet_id` via the pandas `.merge()` function.  \n",
    "\n",
    "The result is a single `master_df` dataframe that has 1473 rows and 15 collumns full of data with the exception of `dog_title` and `second_dog_title` as they are only sometimes observed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
